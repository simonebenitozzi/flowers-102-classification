{"cells":[{"cell_type":"markdown","metadata":{"id":"VYsrgP6Mu-0N"},"source":["# Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0T2NpbRvAcL"},"outputs":[],"source":["from google.colab import drive\n","\n","from os import listdir\n","from os.path import isfile, join\n","import os.path\n","from os import path\n","\n","import tarfile\n","import glob\n","\n","import tensorflow as tf\n","import sklearn \n","from sklearn.decomposition import PCA\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import scipy\n","from scipy import io\n","\n","import PIL\n","from PIL import Image\n","import cv2\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23488,"status":"ok","timestamp":1674399095469,"user":{"displayName":"Karl Lyko","userId":"11096799610030000415"},"user_tz":-60},"id":"3Sc7koT5xkA7","outputId":"52779b12-dbfa-4862-da6c-7f503c8a837d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive', force_remount=True)\n","PATH_PROJ = \"/content/gdrive/MyDrive/AML-proj/\" \n","if not path.exists(PATH_PROJ):\n","    PATH_PROJ = \"/content/gdrive/Shareddrives/AML-proj/\"  # TODO: check if it is correct!\n","\n","PATH_JPG = \"/content/jpg/\"\n","PATH_TAR = PATH_PROJ + \"102flowers.tgz\"\n","IMG_SIZE = 300\n","NUM_CLASSES = 102\n","\n","NAMES = [\n","    \"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\",\n","    \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\",\n","    \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\",\n","    \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\",\n","    \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\",\n","    \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\",\n","    \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\",\n","    \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\",\n","    \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\",\n","    \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\",\n","    \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\",\n","    \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\",\n","    \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\",\n","    \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\",\n","    \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\",\n","    \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\",\n","    \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\",\n","    \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\",\n","    \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\",\n","    \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\",\n","    \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\",\n","    \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\",\n","    \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\",\n","    \"blackberry lily\"\n","]\n","\n","NAMES_ID = dict(zip(NAMES, [x for x in range(len(NAMES))]))\n","ID_NAMES = dict(zip([x for x in range(len(NAMES))], NAMES))"]},{"cell_type":"markdown","metadata":{"id":"Xt7qG6MMr-ia"},"source":["# Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5cVUtKUsBP7"},"outputs":[],"source":["def execute_pca_on_imgs(img, show=False):\n","  # Splitting the image in R,G,B arrays.\n","  b, g, r = cv2.split(img) \n","  #it will split the original image into Blue, Green and Red arrays.\n","\n","  # it is mandatory to do feature scaling before applying PCA because PCA directions are highly sensitive to the relative ranges of features\n","  r_scaled = r / 255\n","  g_scaled = g / 255\n","  b_scaled = b / 255\n","\n","  #initialize PCA with at least 95% variance  \n","  pca_r = PCA(0.95)\n","  pca_r_trans = pca_r.fit_transform(r_scaled)\n","\n","  pca_g = PCA(0.95)\n","  pca_g_trans = pca_g.fit_transform(g_scaled)\n","\n","  pca_b = PCA(0.95)\n","  pca_b_trans = pca_b.fit_transform(b_scaled)\n","\n","  # inverse\n","  pca_r_org = pca_r.inverse_transform(pca_r_trans)\n","  pca_g_org = pca_g.inverse_transform(pca_g_trans)\n","  pca_b_org = pca_b.inverse_transform(pca_b_trans)\n","\n","  # compressiong\n","  img_compressed = cv2.merge((pca_b_org, pca_g_org, pca_r_org))\n","  #viewing the compressed image\n","  if show: \n","    plt.imshow(img_compressed)\n","    plt.show()\n","\n","  return img_compressed\n","    \n","\n","def execute_pca_on_imgs_set(df, path = PATH_JPG):\n","  for img_name in df[\"Id\"]:\n","    RGB_img = plt.imread(path + img_name)\n","    # im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","    im_pca = execute_pca_on_imgs(RGB_img)\n","    cv2.imwrite(PATH_PROJ + \"jpg_pca/\" + img_name, 255*im_pca, [cv2.IMWRITE_JPEG_QUALITY])\n","    \n","\n","def processing_set(dataset, images, labels, size=224, return_pca=False):\n","  x, y = [], []\n","  for num_img in dataset:\n","    # print(f\"linking {num_img} to {images[num_img - 1]}\")\n","    path = PATH_JPG + images[num_img - 1]\n","    im=cv2.imread(path)\n","    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","    im=cv2.resize(im,(size,size))\n","    x.append(im)\n","    y.append(labels[num_img - 1])\n","  \n","  if return_pca:\n","    return execute_pca_on_imgs_set(x), np.asarray(y) \n","  else:\n","    return np.asarray(x), np.asarray(y)\n","\n","\n","def get_all_filenames(tar_fn):\n","    with tarfile.open(tar_fn) as f:\n","        return [m.name for m in f.getmembers() if m.isfile()]\n","\n","\n","def get_img_info(df, path_figure = PATH_JPG):\n","  w, h = [], []\n","  for index, path in enumerate(df[\"Id\"].values):\n","    im = PIL.Image.open(path_figure+ str(path))\n","    w.append(im.size[0])\n","    h.append(im.size[1])\n","\n","  w, h = np.array(w), np.array(h)\n","  return int(np.average(w)), int(np.average(h))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W1SYKYVIvCrg"},"source":["# Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N54AWo0f-Yj8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674399095470,"user_tz":-60,"elapsed":23,"user":{"displayName":"Karl Lyko","userId":"11096799610030000415"}},"outputId":"24d62c00-fb14-4eb5-c116-211f3bdb6675"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/jpg/': No such file or directory\n"]}],"source":["!rm -r $PATH_JPG # remove old data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbZh4WoSvDau"},"outputs":[],"source":["# Import Dataset\n","%%capture\n","!tar -xvf $PATH_TAR -C '/content/'\n","images = [f for f in listdir(PATH_JPG) if isfile(join(PATH_JPG, f))]\n","images = sorted(images)\n","\n","df = pd.DataFrame()\n","df['Id'] = images\n","df['Category'] = scipy.io.loadmat(PATH_PROJ + 'imagelabels.mat')['labels'][0] - 1 \n","df['Category'] = df['Category'].astype(int)\n","#df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1674399102536,"user":{"displayName":"Karl Lyko","userId":"11096799610030000415"},"user_tz":-60},"id":"OT82vpNjos3y","outputId":"eab41dc7-9955-4a65-db82-b29ffc742906"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set: (1020, 2)    Validation set: (1020, 2)    Test set: (6149, 2)\n"]}],"source":["# Split Dataset con le rispettive label\n","split = scipy.io.loadmat(PATH_PROJ + 'setid.mat')\n","test_split = split[\"tstid\"][0] - 1 # start from zero\n","train_split = split[\"trnid\"][0] - 1\n","valid_split = split[\"valid\"][0] - 1\n","\n","train_set = df.iloc[train_split]\n","train_set['Category'].astype(int)\n","test_set = df.iloc[test_split]\n","test_set['Category'].astype(int)\n","val_set = df.iloc[valid_split]\n","val_set['Category'].astype(int)\n","print(\"Train set:\", train_set.shape, \"   Validation set:\", val_set.shape, \"   Test set:\", test_set.shape)"]},{"cell_type":"code","source":["val_set.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"5BjdnjeGY1pn","executionInfo":{"status":"ok","timestamp":1674399102537,"user_tz":-60,"elapsed":12,"user":{"displayName":"Karl Lyko","userId":"11096799610030000415"}},"outputId":"2af43072-a63c-4a11-f62a-6008d9eb802a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   Id  Category\n","6772  image_06773.jpg         0\n","6766  image_06767.jpg         0\n","6738  image_06739.jpg         0\n","6748  image_06749.jpg         0\n","6762  image_06763.jpg         0"],"text/html":["\n","  <div id=\"df-c8854fd9-dc63-4e82-b12f-6168cd191fa6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6772</th>\n","      <td>image_06773.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6766</th>\n","      <td>image_06767.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6738</th>\n","      <td>image_06739.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6748</th>\n","      <td>image_06749.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6762</th>\n","      <td>image_06763.jpg</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8854fd9-dc63-4e82-b12f-6168cd191fa6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c8854fd9-dc63-4e82-b12f-6168cd191fa6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c8854fd9-dc63-4e82-b12f-6168cd191fa6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["val_set.to_csv(PATH_PROJ+\"val_set_df.csv\", index=False)"],"metadata":{"id":"mgDlqKJ8ZCT5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44IMmD3ryff5"},"source":["# Preprocessing & Data Augmentation\n","Data augmentation artificially increases the size of the training set by\n","generating many realistic variants of each training instance. This\n","reduces overfitting, making this a regularization technique. The\n","generated instances should be as realistic as possible: ideally, given an\n","image from the augmented training set, a human should not be able to\n","tell whether it was augmented or not. Simply adding white noise will\n","not help; the modifications should be learnable (white noise is not).\n","\n","For example, you can slightly shift, rotate, and resize every picture in\n","the training set by various amounts and add the resulting pictures to the\n","training set. This forces the model to be more\n","tolerant to variations in the position, orientation, and size of the objects\n","in the pictures. For a model that’s more tolerant of different lighting\n","conditions, you can similarly generate many images with various\n","contrasts. In general, you can also flip the pictures horizontally (except\n","for text, and other asymmetrical objects). By combining these\n","transformations, you can greatly increase the size of your training set.\n","\n","If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean\n","(similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, you could replace each category with a learnable, low-dimensional vector called an embedding. Each category’s representation would be learned during training. This is an example of representation learning"]},{"cell_type":"markdown","metadata":{"id":"GPJk3_7IxD7m"},"source":["**Contrast Augmentation:**\n","\n","LAB color space expresses color variations across three channels. One channel for brightness and two channels for color:\n","- L-channel: representing lightness in the image\n","- a-channel: representing change in color between red and green\n","- b-channel: representing change in color between yellow and blue\n","\n","In the following adaptive histogram equalization id performed on the L-channel and the resulting image is converted back to RGB color space. This enhances the brightness while also limiting contrast sensitivity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhbEtnwFaUs0"},"outputs":[],"source":["def contrast_augmentation(img):\n","\n","    # converting to LAB color space\n","    lab= cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    l_channel, a, b = cv2.split(lab)\n","\n","    # Applying CLAHE to L-channel\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    cl = clahe.apply(l_channel)\n","\n","    # merge the CLAHE enhanced L-channel with the a and b channel\n","    limg = cv2.merge((cl,a,b))\n","\n","    # Converting image from LAB Color model to BGR color spcae\n","    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n","\n","    # Stacking the original image with the enhanced image\n","    return np.hstack((img, enhanced_img))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220383,"status":"ok","timestamp":1674399323260,"user":{"displayName":"Karl Lyko","userId":"11096799610030000415"},"user_tz":-60},"id":"1K9ZVWgPyJux","outputId":"5ec3d4ef-04db-48a8-d6d9-098ec8445978"},"outputs":[{"output_type":"stream","name":"stdout","text":["102  / 102 classes augmented\n","Total Augmented images created=  9180\n"]}],"source":["## remoto to execute\n","# %%script false --no-raise-error\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from IPython.display import clear_output \n","\n","\n","def oversample(df, n, dir, img_size):\n","    \n","    tmp_df=df.copy()\n","    tmp_df[\"Id\"] = dir+tmp_df[\"Id\"]\n","    \n","    # create and store the augmented images  \n","    total=0\n","    gen=ImageDataGenerator(rotation_range = 50,\n","                                shear_range=0.2,\n","                                zoom_range=[0.75,1.25],\n","                                brightness_range=[0.5, 1.5],\n","                                width_shift_range=0.1,\n","                                height_shift_range=0.1,\n","                                horizontal_flip=True)\n","\n","    groups=tmp_df.groupby('Category') # group by class\n","\n","    for label in tmp_df['Category'].unique():  # for every class               \n","        \n","        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n","        sample_count=len(group)   # determine how many samples there are in this class  \n","        \n","        if sample_count< n: # if the class has less than target number of images\n","            \n","            aug_img_count=0\n","            delta=n - sample_count  # number of augmented images to create\n","            \n","            prefix = 'aug-'+str(label).zfill(3)+\"-\"\n","\n","            # augmentation parameters\n","            # The second one, flow_from_dataframe will be very useful to us. \n","            # It checks the path available on the dataframe and then automatically search for the image in train directory. \n","            # Then it make the desired preprocessing steps available in ImageDataGenerator\n","            aug_gen=gen.flow_from_dataframe(group,  \n","                                            x_col='Id', y_col=None, \n","                                            target_size=img_size,\n","                                            class_mode=None, \n","                                            batch_size=1, \n","                                            shuffle=False, \n","                                            save_to_dir=dir, \n","                                            save_prefix=prefix, \n","                                            color_mode='rgb',\n","                                            save_format='jpg')\n","            \n","            # new images creation\n","            while aug_img_count<delta:\n","                images=next(aug_gen)            \n","                aug_img_count += len(images)\n","            total +=aug_img_count\n","\n","            # dataframe updating\n","            for file in os.listdir(dir):\n","                if file.startswith(prefix):\n","                    df.loc[df.index.max()+1] = [file, label]\n","            \n","            clear_output()\n","            print(str(label+1).zfill(3), \" / 102 classes augmented\")\n","\n","    print('Total Augmented images created= ', total)\n","    return df\n","\n","threshold=100\n","img_size=(IMG_SIZE,IMG_SIZE)\n","train_set = oversample(train_set, threshold, \"/content/jpg/\", img_size)"]},{"cell_type":"code","source":["# Copy new augmented dataset into a Drive directory\n","\n","%cp -r /content/jpg /content/gdrive/MyDrive/AML-proj/jpg_augmented300\n","train_set.to_csv(\"/content/gdrive/MyDrive/AML-proj/train_set_augmented300.csv\", index=False)"],"metadata":{"id":"bnQ6POD4oRD2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1R1_3IOy0t_AbC-Tx3vRX_YzGc4xnoD6R","timestamp":1674115457930},{"file_id":"1MDtG_rrgoKlF-sai5qEiISX8818lsbvS","timestamp":1673556329195}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}